{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "from torchvision import models\n",
    "from torchvision import utils\n",
    "import cv2\n",
    "import sys\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import pdb \n",
    "import torch.utils.data as data\n",
    "import h5py\n",
    "from torchvision.transforms import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image as Image\n",
    "%matplotlib inline\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If there is a near-collision in next two seconds or not \n",
    "\n",
    "class FrameDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, f, transform=None, test=False):\n",
    "        \n",
    "        self.f = f \n",
    "        self.transform = transform\n",
    "        self.test = test \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        rgb = np.array(self.f[\"rgb\"][index])\n",
    "        label = np.array(self.f[\"labels\"][index], dtype=np.uint8)\n",
    "        \n",
    "        t_label = torch.zeros(2)\n",
    "        \n",
    "        if (label[0] or label[1]):\n",
    "            t_label[0] = 1 ## Near-collision within next 2 seconds\n",
    "        else:\n",
    "            t_label[1] = 1 ## No Near-collision within next 2 seconds \n",
    "            \n",
    "        t_rgb = torch.zeros(rgb.shape[0], 3, 224, 224)\n",
    "        \n",
    "        prob = random.uniform(0, 1)\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            \n",
    "            for i in range(rgb.shape[0]):\n",
    "                if (prob > 0.5 and not self.test):\n",
    "                    flip_transform = transforms.Compose([transforms.ToPILImage(), transforms.RandomHorizontalFlip(1.0)])\n",
    "                    rgb[i,:,:,:] = flip_transform(rgb[i,:,:,:])\n",
    "                t_rgb[i,:,:,:] = self.transform(rgb[i,:,:,:])\n",
    "                \n",
    "        return rgb, t_rgb, t_label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.f[\"rgb\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "hfp_test = h5py.File('/mnt/hdd1/aashi/cmu_data/threeSecsTest.h5', 'r')\n",
    "test_loader = data.DataLoader(FrameDataset(f = hfp_test, transform = transforms.Compose([transforms.ToTensor(), normalize]), test = True), \n",
    "                               batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vgg_voc_weights(MODEL_PATH):\n",
    "    checkpoint_dict = torch.load(MODEL_PATH)\n",
    "    vgg_model.load_state_dict(checkpoint_dict)\n",
    "\n",
    "vgg_model = models.vgg16(pretrained=True)\n",
    "num_final_in = vgg_model.classifier[-1].in_features\n",
    "NUM_CLASSES = 20 ## in VOC\n",
    "vgg_model.classifier[-1] = nn.Linear(num_final_in, NUM_CLASSES)\n",
    "model_path = '/home/aashi/the_conclusion/model_files/' + 'vgg_on_voc' + str(800)\n",
    "load_vgg_voc_weights(model_path)\n",
    "\n",
    "class VGGNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(VGGNet, self).__init__()\n",
    "        self.rgb_net = self.get_vgg_features()\n",
    "        \n",
    "        kernel_size = 3 \n",
    "        padding = int((kernel_size - 1)/2)\n",
    "        self.conv_layer = nn.Conv2d(512, 16, kernel_size, 1, padding, bias=True)\n",
    "        self.conv_bn = nn.BatchNorm2d(16)\n",
    "        self.feature_size = 16*7*7*4\n",
    "        self.final_layer = nn.Sequential(\n",
    "        nn.Linear(self.feature_size, 256),\n",
    "        nn.Linear(256, 2),  ## 4 classes instead of 2 \n",
    "        nn.Softmax()  ## If loss function uses Softmax  \n",
    "        )\n",
    "        \n",
    "    def forward(self, rgb): ## sequence of four images - last index is latest \n",
    "        four_imgs = []\n",
    "        for i in range(rgb.shape[1]):\n",
    "            img_features = self.rgb_net(rgb[:,i,:,:,:])\n",
    "            channels_reduced = self.conv_bn(self.conv_layer(img_features))\n",
    "            img_features = channels_reduced.view((-1, 16*7*7))\n",
    "            four_imgs.append(img_features)\n",
    "        concat_output = torch.cat(four_imgs, dim = 1)\n",
    "        out = self.final_layer(concat_output)\n",
    "        return out\n",
    "        \n",
    "    def get_vgg_features(self):\n",
    "\n",
    "        modules = list(vgg_model.children())[:-1]\n",
    "        vgg16 = nn.Sequential(*modules)\n",
    "        \n",
    "        return vgg16.type(torch.Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_weights(epoch_num):\n",
    "    model_file = '/mnt/hdd1/aashi/binary_classification_' + str(epoch_num).zfill(3)\n",
    "    checkpoint_dict = torch.load(model_file)\n",
    "    model.load_state_dict(checkpoint_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor():\n",
    "    \"\"\" Class for extracting activations and \n",
    "    registering gradients from targetted intermediate layers \"\"\"\n",
    "    def __init__(self, model, target_layers):\n",
    "        self.model = model\n",
    "        self.target_layers = target_layers\n",
    "        self.gradients = []\n",
    "\n",
    "    def save_gradient(self, grad):\n",
    "    \tself.gradients.append(grad)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outputs = []\n",
    "        self.gradients = []\n",
    "        for name, module in self.model._modules.items():\n",
    "            x = module(x)\n",
    "            if name in self.target_layers:\n",
    "                x.register_hook(self.save_gradient)\n",
    "                outputs += [x]\n",
    "        return outputs, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelOutputs():\n",
    "    \"\"\" Class for making a forward pass, and getting:\n",
    "    1. The network output.\n",
    "    2. Activations from intermeddiate targetted layers.\n",
    "    3. Gradients from intermeddiate targetted layers. \"\"\"\n",
    "    def __init__(self, model, target_layers):\n",
    "        self.model = model\n",
    "        self.feature_extractor = FeatureExtractor(self.model.rgb_net, target_layers)\n",
    "\n",
    "    def get_gradients(self):\n",
    "        return self.feature_extractor.gradients\n",
    "\n",
    "    def __call__(self, x): ## x is a sequence of four imgs [1,4,3,224,224] \n",
    "        four_imgs = []\n",
    "        target_activations, _ = self.feature_extractor(x[:,0,:,:,:])\n",
    "\n",
    "        for i in range(x.shape[1]):\n",
    "            _, output  = self.feature_extractor(x[:,i,:,:,:])\n",
    "\n",
    "            channels_reduced = self.model.conv_layer(output)\n",
    "            img_features = channels_reduced.view((-1, 16*7*7))\n",
    "            four_imgs.append(img_features)\n",
    "\n",
    "        concat_output = torch.cat(four_imgs, dim = 1)\n",
    "\n",
    "        out = self.model.final_layer(concat_output)\n",
    "\n",
    "        return target_activations, out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cam_on_img(img, mask, seq):\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255*mask), cv2.COLORMAP_JET)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "    cam = heatmap + np.float32(img)\n",
    "    cam = cam / np.max(cam)\n",
    "    cv2.imwrite(\"heatmaps/\" + str(iter+1) + \".jpg\", np.uint8(255 * cam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCam:\n",
    "\tdef __init__(self, model, target_layer_names, use_cuda):\n",
    "\t\tself.model = model\n",
    "\t\tself.model.eval()\n",
    "\t\tself.cuda = use_cuda\n",
    "\t\tif self.cuda:\n",
    "\t\t\tself.model = model.cuda()\n",
    "\n",
    "\t\tself.extractor = ModelOutputs(self.model, target_layer_names)\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\treturn self.model(input) \n",
    "\n",
    "\tdef __call__(self, input, index = None):\n",
    "\t\tif self.cuda:\n",
    "\t\t\tfeatures, output = self.extractor(input.cuda())\n",
    "\t\telse:\n",
    "\t\t\tfeatures, output = self.extractor(input)\n",
    "\n",
    "\t\tif index == None:\n",
    "\t\t\tindex = np.argmax(output.cpu().data.numpy())\n",
    "\n",
    "\t\tone_hot = np.zeros((1, output.size()[-1]), dtype = np.float32)\n",
    "\t\tone_hot[0][index] = 1\n",
    "\t\tone_hot = Variable(torch.from_numpy(one_hot), requires_grad = True)\n",
    "\t\tif self.cuda:\n",
    "\t\t\tone_hot = torch.sum(one_hot.cuda() * output)\n",
    "\t\telse:\n",
    "\t\t\tone_hot = torch.sum(one_hot * output)\n",
    "\n",
    "\t\tself.model.rgb_net.zero_grad()\n",
    "\t\tself.model.conv_layer.zero_grad()\n",
    "\t\tself.model.final_layer.zero_grad()\n",
    "\t\tone_hot.backward()\n",
    "\n",
    "\t\tgrads_val = self.extractor.get_gradients()[-1].cpu().data.numpy()\n",
    "\n",
    "\t\ttarget = features[-1]\n",
    "\t\ttarget = target.cpu().data.numpy()[0, :]\n",
    "\n",
    "\t\tweights = np.mean(grads_val, axis = (2, 3))[0, :]\n",
    "\t\tcam = np.zeros(target.shape[1 : ], dtype = np.float32)\n",
    "\n",
    "\t\tfor i, w in enumerate(weights):\n",
    "\t\t\tcam += w * target[i, :, :]        \n",
    "\n",
    "\t\tcam = np.maximum(cam, 0)        ## ReLU here ## only the pixels which have positive influence on the class \n",
    "\t\tcam = cv2.resize(cam, (224, 224))\n",
    "\t\tcam = cam - np.min(cam) ## Normalized for visualization purpose \n",
    "\t\tcam = cam / np.max(cam)\n",
    "\t\treturn cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aashi/anaconda3/envs/cenv/lib/python3.6/site-packages/torch/nn/modules/container.py:91: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    }
   ],
   "source": [
    "use_cuda = 1 \n",
    "\n",
    "model = VGGNet()\n",
    "\n",
    "grad_cam = GradCam(model = model, target_layer_names = [\"0\"], use_cuda=use_cuda)\n",
    "\n",
    "\n",
    "e = 2\n",
    "load_model_weights(2)\n",
    "\n",
    "target_index = None \n",
    "\n",
    "for iter, (rgb, t_rgb, label) in enumerate(test_loader, 0):\n",
    "    \n",
    "    input = t_rgb.float().cuda()\n",
    "    mask = grad_cam(input, target_index)\n",
    "    vrgb = rgb[0,3,:,:,:] ## heatmap of last image \n",
    "    \n",
    "    img = np.ascontiguousarray(vrgb)\n",
    "    \n",
    "    show_cam_on_img(img/255, mask, iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
