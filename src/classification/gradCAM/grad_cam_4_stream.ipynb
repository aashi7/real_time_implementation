{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aashi/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Function\n",
    "from torchvision import models\n",
    "from torchvision import utils\n",
    "import cv2\n",
    "import sys\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import pdb \n",
    "import torch.utils.data as data\n",
    "import h5py\n",
    "from torchvision.transforms import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL.Image as Image\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, f, transform=None):\n",
    "        self.f = f \n",
    "        self.transform = transform \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        rgb = np.array(self.f[\"rgb\"][index])\n",
    "        label = np.array(self.f[\"labels\"][index], dtype=np.uint8)\n",
    "#         scene_num = self.f[\"scene_num\"][index]\n",
    "#         frame_num = self.f[\"frame_num\"][index]\n",
    "        \n",
    "        if (label > 0):\n",
    "            label = 1\n",
    "        \n",
    "        ## label value transformed to binary vector\n",
    "        t_label = torch.zeros(2)\n",
    "        t_label[label] = 1\n",
    "        \n",
    "        t_rgb = torch.zeros(rgb.shape[0], 3, 224, 224)\n",
    "        \n",
    "        if self.transform is not None: ## Have to create a video for non transform \n",
    "            for i in range(rgb.shape[0]):\n",
    "#                 img_to_resize = rgb[i,:,:,:]\n",
    "#                 img_to_resize = Image.fromarray(img_to_resize.astype('uint8'), 'RGB')\n",
    "#                 img_to_resize = img_to_resize.resize((224, 224))    \n",
    "#                 img_to_resize = self.transform(img_to_resize)\n",
    "                t_rgb[i,:,:,:] = self.transform(rgb[i,:,:,:])\n",
    "                \n",
    "#         return rgb, t_rgb, scene_num, frame_num, t_label ## also returning rgb for visualization \n",
    "        return rgb, t_rgb, t_label \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.f[\"rgb\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vgg_voc_weights(MODEL_PATH):\n",
    "    checkpoint_dict = torch.load(MODEL_PATH)\n",
    "    vgg_model.load_state_dict(checkpoint_dict)\n",
    "\n",
    "vgg_model = models.vgg16(pretrained=True)\n",
    "num_final_in = vgg_model.classifier[-1].in_features\n",
    "NUM_CLASSES = 20 ## in VOC\n",
    "vgg_model.classifier[-1] = nn.Linear(num_final_in, NUM_CLASSES)\n",
    "model_path = '/home/aashi/the_conclusion/model_files/' + 'vgg_on_voc' + str(800)\n",
    "load_vgg_voc_weights(model_path)\n",
    "\n",
    "class VGGNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(VGGNet, self).__init__()\n",
    "        self.rgb_net = self.get_vgg_features()\n",
    "        \n",
    "        kernel_size = 3 \n",
    "        padding = int((kernel_size - 1)/2)\n",
    "        self.conv_layer = nn.Conv2d(512, 16, kernel_size, 1, padding, bias=True)\n",
    "        ## input_channels, output_channels, kernel_size, stride, padding, bias\n",
    "        self.feature_size = 16*7*7*4\n",
    "        self.final_layer = nn.Sequential(\n",
    "        nn.Linear(self.feature_size, 256),\n",
    "        nn.Linear(256, 2),\n",
    "        #nn.Sigmoid()\n",
    "        nn.Softmax() \n",
    "        )\n",
    "        \n",
    "    def forward(self, rgb): ## sequence of four images - last index is latest \n",
    "        four_imgs = []\n",
    "        for i in range(rgb.shape[1]):\n",
    "            img_features = self.rgb_net(rgb[:,i,:,:,:])\n",
    "            channels_reduced = self.conv_layer(img_features)\n",
    "            img_features = channels_reduced.view((-1, 16*7*7))\n",
    "            four_imgs.append(img_features)\n",
    "        concat_output = torch.cat(four_imgs, dim = 1)\n",
    "        out = self.final_layer(concat_output)\n",
    "        return out\n",
    "#         return concat_output\n",
    "        \n",
    "    def get_vgg_features(self):\n",
    "\n",
    "        ##vgg16 = load_vgg_voc_weights(vgg16, model_path)\n",
    "        modules = list(vgg_model.children())[:-1]\n",
    "        ## I can also freeze \n",
    "        ## high level layer, should I take a lower level?\n",
    "        vgg16 = nn.Sequential(*modules)\n",
    "        \n",
    "        ## Uncommented this to let it fine-tune on my model \n",
    "        # for p in vgg16.parameters():\n",
    "        #     p.requires_grad = False \n",
    "        \n",
    "        return vgg16.type(torch.Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor():\n",
    "    \"\"\" Class for extracting activations and \n",
    "    registering gradients from targetted intermediate layers \"\"\"\n",
    "    def __init__(self, model, target_layers):\n",
    "        self.model = model\n",
    "        self.target_layers = target_layers\n",
    "        self.gradients = []\n",
    "\n",
    "    def save_gradient(self, grad):\n",
    "    \tself.gradients.append(grad)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        outputs = []\n",
    "        self.gradients = []\n",
    "        #pdb.set_trace()\n",
    "        for name, module in self.model._modules.items():\n",
    "            #pdb.set_trace()\n",
    "            x = module(x)\n",
    "            if name in self.target_layers:\n",
    "                x.register_hook(self.save_gradient)\n",
    "                outputs += [x]\n",
    "        return outputs, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelOutputs():\n",
    "    \"\"\" Class for making a forward pass, and getting:\n",
    "    1. The network output.\n",
    "    2. Activations from intermeddiate targetted layers.\n",
    "    3. Gradients from intermeddiate targetted layers. \"\"\"\n",
    "    def __init__(self, model, target_layers):\n",
    "        self.model = model\n",
    "        self.feature_extractor = FeatureExtractor(self.model.rgb_net, target_layers)\n",
    "\n",
    "    def get_gradients(self):\n",
    "        return self.feature_extractor.gradients\n",
    "\n",
    "    def __call__(self, x): ## x is a sequence of four imgs [1,4,3,224,224] \n",
    "        four_imgs = []\n",
    "        target_activations, _ = self.feature_extractor(x[:,0,:,:,:])\n",
    "#         pdb.set_trace()\n",
    "        for i in range(x.shape[1]):\n",
    "            _, output  = self.feature_extractor(x[:,i,:,:,:])\n",
    "            #pdb.set_trace()\n",
    "            channels_reduced = self.model.conv_layer(output)\n",
    "            img_features = channels_reduced.view((-1, 16*7*7))\n",
    "            four_imgs.append(img_features)\n",
    "        #pdb.set_trace()\n",
    "        concat_output = torch.cat(four_imgs, dim = 1)\n",
    "#             output = output.view(output.size(0), -1)\n",
    "        out = self.model.final_layer(concat_output) ## Have to fix the\n",
    "#         print(out)\n",
    "        return target_activations, out ## target_activation of latest image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_cam_on_image(img, mask, seq):\n",
    "\theatmap = cv2.applyColorMap(np.uint8(255*mask), cv2.COLORMAP_JET)\n",
    "\theatmap = np.float32(heatmap) / 255\n",
    "\tcam = heatmap + np.float32(img)\n",
    "\tcam = cam / np.max(cam)\n",
    "# \tpdb.set_trace()    \n",
    "# \tcv2.imwrite(\"heatmaps/\" + str(iter + 1) + \".jpg\", np.uint8(255 * cam))\n",
    "# \tcv2.imwrite(\"heatmaps_test_seq_2/\" + str(iter+1) + \".jpg\", np.uint8(255 * cam))\n",
    "\tcv2.imwrite(\"heatmaps_paper/\" + str(iter+1) + \".jpg\", np.uint8(255 * cam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuidedBackpropReLU(Function):\n",
    "\n",
    "    def forward(self, input):\n",
    "        positive_mask = (input > 0).type_as(input)\n",
    "        output = torch.addcmul(torch.zeros(input.size()).type_as(input), input, positive_mask)\n",
    "        self.save_for_backward(input, output)\n",
    "        return output\n",
    "\n",
    "    def backward(self, grad_output):\n",
    "        input, output = self.saved_tensors\n",
    "        grad_input = None\n",
    "\n",
    "        positive_mask_1 = (input > 0).type_as(grad_output)\n",
    "        positive_mask_2 = (grad_output > 0).type_as(grad_output)\n",
    "        grad_input = torch.addcmul(torch.zeros(input.size()).type_as(input), torch.addcmul(torch.zeros(input.size()).type_as(input), grad_output, positive_mask_1), positive_mask_2)\n",
    "\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuidedBackpropReLUModel:\n",
    "\tdef __init__(self, model, use_cuda):\n",
    "\t\tself.model = model\n",
    "\t\tself.model.eval()\n",
    "\t\tself.cuda = use_cuda\n",
    "\t\tif self.cuda:\n",
    "\t\t\tself.model = model.cuda()\n",
    "\n",
    "\t\t# replace ReLU with GuidedBackpropReLU\n",
    "\t\tfor idx, module in self.model.features._modules.items():\n",
    "\t\t\tif module.__class__.__name__ == 'ReLU':\n",
    "\t\t\t\tself.model.features._modules[idx] = GuidedBackpropReLU()\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\treturn self.model(input)\n",
    "\n",
    "\tdef __call__(self, input, index = None):\n",
    "\t\tif self.cuda:\n",
    "\t\t\toutput = self.forward(input.cuda())\n",
    "\t\telse:\n",
    "\t\t\toutput = self.forward(input)\n",
    "\n",
    "\t\tif index == None:\n",
    "\t\t\tindex = np.argmax(output.cpu().data.numpy())\n",
    "\n",
    "\t\tone_hot = np.zeros((1, output.size()[-1]), dtype = np.float32)\n",
    "\t\tone_hot[0][index] = 1\n",
    "\t\tone_hot = Variable(torch.from_numpy(one_hot), requires_grad = True)\n",
    "\t\tif self.cuda:\n",
    "\t\t\tone_hot = torch.sum(one_hot.cuda() * output)\n",
    "\t\telse:\n",
    "\t\t\tone_hot = torch.sum(one_hot * output)\n",
    "\n",
    "\t\t# self.model.features.zero_grad()\n",
    "\t\t# self.model.classifier.zero_grad()\n",
    "\t\tone_hot.backward()\n",
    "\n",
    "\t\toutput = input.grad.cpu().data.numpy()\n",
    "\t\toutput = output[0,:,:,:]\n",
    "\n",
    "\t\treturn output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCam:\n",
    "\tdef __init__(self, model, target_layer_names, use_cuda):\n",
    "\t\tself.model = model\n",
    "\t\tself.model.eval()\n",
    "\t\tself.cuda = use_cuda\n",
    "\t\tif self.cuda:\n",
    "\t\t\tself.model = model.cuda()\n",
    "\n",
    "\t\tself.extractor = ModelOutputs(self.model, target_layer_names)\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\treturn self.model(input) \n",
    "\n",
    "\tdef __call__(self, input, index = None):\n",
    "\t\tif self.cuda:\n",
    "\t\t\tfeatures, output = self.extractor(input.cuda())\n",
    "\t\telse:\n",
    "\t\t\tfeatures, output = self.extractor(input)\n",
    "\n",
    "\t\tif index == None:\n",
    "\t\t\tindex = np.argmax(output.cpu().data.numpy())\n",
    "\n",
    "\t\tone_hot = np.zeros((1, output.size()[-1]), dtype = np.float32)\n",
    "\t\tone_hot[0][index] = 1\n",
    "\t\tone_hot = Variable(torch.from_numpy(one_hot), requires_grad = True)\n",
    "\t\tif self.cuda:\n",
    "\t\t\tone_hot = torch.sum(one_hot.cuda() * output)\n",
    "\t\telse:\n",
    "\t\t\tone_hot = torch.sum(one_hot * output)\n",
    "\n",
    "\t\tself.model.rgb_net.zero_grad()\n",
    "\t\tself.model.conv_layer.zero_grad()\n",
    "\t\tself.model.final_layer.zero_grad()\n",
    "\t\tone_hot.backward()\n",
    "\n",
    "\t\tgrads_val = self.extractor.get_gradients()[-1].cpu().data.numpy()\n",
    "\n",
    "\t\ttarget = features[-1]\n",
    "\t\ttarget = target.cpu().data.numpy()[0, :]\n",
    "\n",
    "\t\tweights = np.mean(grads_val, axis = (2, 3))[0, :]\n",
    "\t\tcam = np.zeros(target.shape[1 : ], dtype = np.float32)\n",
    "\n",
    "\t\tfor i, w in enumerate(weights):\n",
    "\t\t\tcam += w * target[i, :, :]        \n",
    "\n",
    "\t\tcam = np.maximum(cam, 0)        ## ReLU here ## only the pixels which have positive influence on the class \n",
    "\t\tcam = cv2.resize(cam, (224, 224))\n",
    "\t\tcam = cam - np.min(cam) ## Normalized for visualization purpose \n",
    "\t\tcam = cam / np.max(cam)\n",
    "\t\treturn cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(imgs):\n",
    "    \n",
    "    t_rgb = torch.zeros(4,3,224,224)\n",
    "\n",
    "    means=[0.485, 0.456, 0.406]\n",
    "    stds=[0.229, 0.224, 0.225]\n",
    "\n",
    "    for i in range(4):\n",
    "        img = imgs[i]\n",
    "        preprocessed_img = img.copy()[: , :, ::-1]\n",
    "        for i in range(3):\n",
    "            preprocessed_img[:, :, i] = preprocessed_img[:, :, i] - means[i]\n",
    "            preprocessed_img[:, :, i] = preprocessed_img[:, :, i] / stds[i]\n",
    "        preprocessed_img = \\\n",
    "            np.ascontiguousarray(np.transpose(preprocessed_img, (2, 0, 1)))\n",
    "        preprocessed_img = torch.from_numpy(preprocessed_img)\n",
    "        #preprocessed_img.unsqueeze_(0)\n",
    "        t_rgb[i,:,:,:] = preprocessed_img \n",
    "    t_rgb.unsqueeze_(0)\n",
    "    input = Variable(t_rgb, requires_grad = True)\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "\tparser = argparse.ArgumentParser()\n",
    "\tparser.add_argument('--use-cuda', action='store_true', default=False,\n",
    "\t                    help='Use NVIDIA GPU acceleration')\n",
    "\tparser.add_argument('--image-path', type=str, default='./examples/both.png',\n",
    "\t                    help='Input image path')\n",
    "\targs = parser.parse_args()\n",
    "\targs.use_cuda = args.use_cuda and torch.cuda.is_available()\n",
    "\tif args.use_cuda:\n",
    "\t    print(\"Using GPU for acceleration\")\n",
    "\telse:\n",
    "\t    print(\"Using CPU for computation\")\n",
    "\n",
    "\treturn args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_weights(MODEL_PATH):\n",
    "    checkpoint_dict = torch.load(MODEL_PATH)\n",
    "    model.load_state_dict(checkpoint_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGGNet(\n",
      "  (rgb_net): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU(inplace)\n",
      "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (3): ReLU(inplace)\n",
      "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (6): ReLU(inplace)\n",
      "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (8): ReLU(inplace)\n",
      "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): ReLU(inplace)\n",
      "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (13): ReLU(inplace)\n",
      "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (15): ReLU(inplace)\n",
      "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (18): ReLU(inplace)\n",
      "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (20): ReLU(inplace)\n",
      "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (22): ReLU(inplace)\n",
      "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (25): ReLU(inplace)\n",
      "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (27): ReLU(inplace)\n",
      "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (29): ReLU(inplace)\n",
      "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      "  (conv_layer): Conv2d(512, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (final_layer): Sequential(\n",
      "    (0): Linear(in_features=3136, out_features=256, bias=True)\n",
      "    (1): Linear(in_features=256, out_features=2, bias=True)\n",
      "    (2): Softmax()\n",
      "  )\n",
      ")\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aashi/anaconda3/envs/cenv/lib/python3.6/site-packages/torch/nn/modules/container.py:91: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "> <ipython-input-13-3da5dc6e65ef>(105)<module>()\n",
      "-> for iter, (rgb, t_rgb, label) in enumerate(test_loader, 0):\n",
      "(Pdb) q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3da5dc6e65ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m# show_cam_on_image(img1, mask, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_rgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_rgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_cam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-3da5dc6e65ef>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;31m# show_cam_on_image(img1, mask, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_rgb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt_rgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_cam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" python grad_cam.py <path_to_image>\n",
    "1. Loads an image with opencv.\n",
    "2. Preprocesses it for VGG19 and converts to a pytorch variable.\n",
    "3. Makes a forward pass to find the category index with the highest score,\n",
    "and computes intermediate activations.\n",
    "Makes the visualization. \"\"\"\n",
    "\n",
    "# args = get_args()\n",
    "use_cuda = 1\n",
    "image_path_1 = 'examples/611.png'\n",
    "image_path_2 = 'examples/612.png'\n",
    "image_path_3 = 'examples/613.png'\n",
    "image_path_4 = 'examples/614.png'\n",
    "\n",
    "# Can work with any model, but it assumes that the model has a \n",
    "# feature method, and a classifier method,\n",
    "# as in the VGG models in torchvision.\n",
    "\n",
    "# object_categories = ['aeroplane', 'bicycle', 'bird', 'boat',\n",
    "#                      'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "#                      'cow', 'diningtable', 'dog', 'horse',\n",
    "#                      'motorbike', 'person', 'pottedplant',\n",
    "#                      'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "\n",
    "# model = models.vgg16(pretrained=True)\n",
    "model = VGGNet()\n",
    "# num_final_in = model.classifier[-1].in_features\n",
    "# NUM_CLASSES = len(object_categories)\n",
    "# model.classifier[-1] = nn.Linear(num_final_in, NUM_CLASSES)\n",
    "\n",
    "## model = model.cuda()\n",
    "\n",
    "######## I deleted the lidar_jitter model files ########\n",
    "# model_path = '/mnt/hdd1/aashi/lidar_jitter_025'  ## Unfortunately I deleted this \n",
    "\n",
    "## New Model trained with similar loss \n",
    "model_path = '/mnt/hdd1/aashi/lidar_jitter_014'\n",
    "# model_path = '/home/aashi/the_conclusion/model_files/' + 'vgg_on_voc' + str(800)\n",
    "load_model_weights(model_path)\n",
    "\n",
    "# grad_cam = GradCam(model = models.vgg19(pretrained=True), \\\n",
    "# \t\t\t\ttarget_layer_names = [\"35\"], use_cuda=args.use_cuda)\n",
    "\n",
    "print(model)\n",
    "\n",
    "# model2 =  models.vgg19(pretrained=True)\n",
    "# print(model2)\n",
    "\n",
    "grad_cam = GradCam(model = model, target_layer_names = [\"0\"], use_cuda=use_cuda)\n",
    "\n",
    "# img = cv2.imread(image_path, 1)\n",
    "# img = np.float32(cv2.resize(img, (224, 224))) / 255\n",
    "# input = preprocess_image(img)\n",
    "\n",
    "# list_of_imgs = []\n",
    "# img = cv2.imread(image_path_1, 1)\n",
    "# img = np.float32(cv2.resize(img, (224, 224))) / 255\n",
    "# #img = np.fliplr(img)\n",
    "# img1 = img\n",
    "# list_of_imgs.append(img)\n",
    "\n",
    "# img = cv2.imread(image_path_2, 1)\n",
    "# img = np.float32(cv2.resize(img, (224, 224))) / 255\n",
    "# #img = np.fliplr(img)\n",
    "# img2 = img\n",
    "# list_of_imgs.append(img)\n",
    "\n",
    "# img = cv2.imread(image_path_3, 1)\n",
    "# img = np.float32(cv2.resize(img, (224, 224))) / 255\n",
    "# #img = np.fliplr(img)\n",
    "# img3 = img\n",
    "# list_of_imgs.append(img)\n",
    "\n",
    "# img = cv2.imread(image_path_4, 1)\n",
    "# img = np.float32(cv2.resize(img, (224, 224))) / 255\n",
    "# #img = np.fliplr(img)\n",
    "# img4 = img\n",
    "# list_of_imgs.append(img)\n",
    "# input = preprocess_image(list_of_imgs)\n",
    "\n",
    "# hfp_test = h5py.File('/mnt/hdd1/aashi/cmu_data/complete_data_test.h5', 'r')\n",
    "\n",
    "\n",
    "hfp_test = h5py.File('/mnt/hdd1/aashi/cmu_data/test_seq_2.h5', 'r')\n",
    "\n",
    "# hfp_test = h5py.File('/mnt/hdd1/aashi/cmu_data/test_seq_2_high_res.h5', 'r')\n",
    "# grad CAM not good \n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "batch_size = 1\n",
    "transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "test_loader = data.DataLoader(FrameDataset(f = hfp_test, transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), normalize])), batch_size = batch_size)\n",
    "\n",
    "# If None, returns the map for the highest scoring category.\n",
    "# Otherwise, targets the requested index.\n",
    "#target_index = None\n",
    "# Changing the target index from None to 1; 1 corresponds to pred = 1 \n",
    "# changes the target to None - backpropragate high prob score \n",
    "target_index = None \n",
    "\n",
    "# mask = grad_cam(input, target_index)\n",
    "# show_cam_on_image(img1, mask, 1)\n",
    "\n",
    "for iter, (rgb, t_rgb, label) in enumerate(test_loader, 0):\n",
    "    input = t_rgb.float().cuda()\n",
    "    mask = grad_cam(input, target_index)\n",
    "    vrgb = rgb[0,3,:,:,:]  ## heatmap of last image \n",
    "    img = np.ascontiguousarray(vrgb)\n",
    "\n",
    "#     resized_mask = cv2.resize(mask, (1280, 720))\n",
    "#     show_cam_on_image(img/255, resized_mask, iter)\n",
    "    show_cam_on_image(img/255, mask, iter)\n",
    "    print(iter)\n",
    "    if (iter == 65):\n",
    "        pdb.set_trace()\n",
    "# pdb.set_trace()\n",
    "\n",
    "# gb_model = GuidedBackpropReLUModel(model = models.vgg19(pretrained=True), use_cuda=use_cuda)\n",
    "# gb = gb_model(input, index=target_index)\n",
    "# utils.save_image(torch.from_numpy(gb), 'gb2.jpg')\n",
    "\n",
    "# cam_mask = np.zeros(gb.shape)\n",
    "# for i in range(0, gb.shape[0]):\n",
    "#     cam_mask[i, :, :] = mask\n",
    "\n",
    "# cam_gb = np.multiply(cam_mask, gb)\n",
    "# utils.save_image(torch.from_numpy(cam_gb), 'cam_gb2.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## What is my classification layer? ###########\n",
    "\n",
    "########## This is BINARY CLASSIFICATION ###########\n",
    "\n",
    "# hfp_test = h5py.File('/mnt/hdd1/aashi/cmu_data/test_seq_2.h5', 'r')\n",
    "\n",
    "# normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "# batch_size = 1\n",
    "# transform = transforms.Compose([transforms.ToTensor(), normalize])\n",
    "# test_loader = data.DataLoader(FrameDataset(f = hfp_test, transform = transforms.Compose(\n",
    "#     [transforms.ToTensor(), normalize])), batch_size = batch_size)\n",
    "\n",
    "# for iter, (rgb, t_rgb, label) in enumerate(test_loader, 0):\n",
    "#     pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
